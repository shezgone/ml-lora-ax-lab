{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e8c7b0",
   "metadata": {},
   "source": [
    "# Gemma-2-27B: Base vs Fine-tuned Comparison\n",
    "\n",
    "This notebook compares the performance of the base `mlx-community/gemma-2-27b-it-4bit` model with the fine-tuned version (LoRA adapters).\n",
    "We check for:\n",
    "1. **Knowledge Injection:** Does it know about \"SolverX\"?\n",
    "2. **General Capabilities:** Does it still answer general Python/Common Sense questions correctly (avoiding Catastrophic Forgetting)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56976cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"mlx-community/gemma-2-27b-it-4bit\"\n",
    "ADAPTER_PATH = \"adapters_27b\"  # Path to the fine-tuned adapters\n",
    "\n",
    "# Test Questions (Mixed Domain)\n",
    "questions = [\n",
    "    # Domain Specific (SolverX)\n",
    "    \"SolverX가 뭐야?\",\n",
    "    \"SolverX의 본사는 어디에 있어?\",\n",
    "    \"SolverX Fusion의 특징은?\",\n",
    "    \n",
    "    # General Knowledge (Python)\n",
    "    \"파이썬에서 리스트를 정렬하는 함수는?\",\n",
    "    \"파이썬으로 Hello World 출력하는 코드를 짜줘.\",\n",
    "    \n",
    "    # General Knowledge (Common Sense)\n",
    "    \"대한민국의 수도는 어디야?\",\n",
    "    \"사과는 무슨 색이야?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate Base Model\n",
    "print(f\"Loading Base Model: {MODEL_PATH}\")\n",
    "model, tokenizer = load(MODEL_PATH)\n",
    "\n",
    "base_responses = []\n",
    "print(\"Generating responses with Base Model...\")\n",
    "for q in questions:\n",
    "    prompt = f\"<start_of_turn>user\\n{q}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    response = generate(model, tokenizer, prompt=prompt, max_tokens=200, verbose=False)\n",
    "    base_responses.append(response.strip())\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "print(\"\\nBase Model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ecf4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Evaluate Fine-tuned Model\n",
    "print(f\"Loading Fine-tuned Model with Adapters: {ADAPTER_PATH}\")\n",
    "# Reload model with adapters\n",
    "model, tokenizer = load(MODEL_PATH, adapter_path=ADAPTER_PATH)\n",
    "\n",
    "ft_responses = []\n",
    "print(\"Generating responses with Fine-tuned Model...\")\n",
    "for q in questions:\n",
    "    prompt = f\"<start_of_turn>user\\n{q}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    response = generate(model, tokenizer, prompt=prompt, max_tokens=200, verbose=False)\n",
    "    ft_responses.append(response.strip())\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "print(\"\\nFine-tuned Model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bc081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compare Results\n",
    "results = []\n",
    "for i, q in enumerate(questions):\n",
    "    results.append({\n",
    "        \"Question\": q,\n",
    "        \"Base Model\": base_responses[i],\n",
    "        \"Fine-tuned Model\": ft_responses[i]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Style the dataframe for better readability\n",
    "def highlight_diff(row):\n",
    "    # Simple heuristic: if FT is different from Base, highlight it?\n",
    "    # Or just return empty styles.\n",
    "    return ['' for _ in row]\n",
    "\n",
    "styled_df = df.style.set_properties(**{'text-align': 'left', 'white-space': 'pre-wrap'}) \\\n",
    "    .set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "\n",
    "display(HTML(\"<h3>Comparison Results</h3>\"))\n",
    "display(styled_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
